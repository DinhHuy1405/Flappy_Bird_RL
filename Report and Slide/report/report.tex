\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Comparative Analysis of Q-learning and SARSA in the Flappy Bird Game with Enhanced Reward Functions}

\author{
    \IEEEauthorblockN{Phan Duy Kha, Nguyen Dinh Huy}
    % \IEEEauthorblockA{
    %     Affiliation \\
    %     Email: \{phan.duykha, nguyen.dinhhuy\}@example.com}
}

\maketitle

\begin{abstract}
This report analyzes the performance of Q-learning and SARSA in training an agent for the Flappy Bird game. An enhanced reward function is utilized to encourage prolonged flight, reward bonus actions, and penalize collisions. The report describes the game mechanics, detailed state representation, action dynamics, reward structure, learning algorithms, experimental evaluation, and discussion on results and future improvements.
\end{abstract}

\begin{IEEEkeywords}
Reinforcement Learning, Q-learning, SARSA, Flappy Bird, Reward Functions.
\end{IEEEkeywords}

\section{Introduction}
This report analyzes the performance of Q-learning and SARSA in training an agent for the Flappy Bird game. An enhanced reward function is utilized to encourage prolonged flight, reward bonus actions, and penalize collisions. The structure of the report includes a description of the game mechanics, a detailed analysis of state representation, actions, and reward functions, followed by the methodology, experimental evaluation, and discussion on results and future improvements.

\section{Game Description and Mechanics}
The Flappy Bird game offers a challenging environment for Reinforcement Learning experiments. Implemented using \texttt{pygame}, it comprises several key components that simulate realistic game dynamics.

\subsection{Game Elements and Dynamics}
\begin{itemize}
    \item \textbf{Visual Elements:} The game features a background, moving ground, pipes as obstacles, and a bird controlled by the agent. A bonus element, referred to as a “gift”, is also included.
    \item \textbf{Dynamics:}
    \begin{itemize}
        \item The bird is affected by gravity and can flap to gain altitude.
        \item Pipes continuously move leftwards at a constant speed, with new pipes generated as the older ones exit the screen.
        \item The gift moves along with the pipes; when collected, it provides an extra reward.
    \end{itemize}
\end{itemize}

\subsection{State Representation and Actions}
The agent's perception of the environment is defined by its state representation and the available actions.

\subsubsection{State Representation}
The state of the game is represented as a tuple capturing essential information related to the bird's position and the environment:
\begin{itemize}
    \item \textbf{Horizontal Distance to the Next Pipe:} This is calculated as the difference in x-coordinates between the bird and the next set of pipes, normalized by a fixed factor to maintain consistency.
    \item \textbf{Vertical Distance to the Next Pipe:} This component measures the difference in height between the bird and the lower edge of the upper pipe, with adjustments to ensure all values are non-negative.
    \item \textbf{Position of the Next Gift:} Both the x and y coordinates of the gift are incorporated; the x-coordinate is normalized similarly to that of the pipes, while the y-coordinate is computed relative to the bird's current vertical position.
\end{itemize}
This detailed representation ensures the agent captures all relevant spatial information to make informed decisions.

\subsubsection{Actions}
The agent has two possible actions:
\begin{itemize}
    \item \textbf{Action 0 (Do Nothing):} The bird is allowed to descend naturally under gravity.
    \item \textbf{Action 1 (Flap):} The bird flaps its wings to gain altitude, counteracting gravity.
\end{itemize}
Action selection is implemented using a greedy or \(\epsilon\)-greedy strategy, where the agent occasionally takes a random action to promote exploration while predominantly choosing the action with the highest expected reward.

\subsection{Enhanced Reward Function}
The reward function is designed to provide clear, multidimensional feedback to guide the agent’s learning:
\begin{itemize}
    \item \textbf{+200:} Awarded for successfully passing through a pipe.
    \item \textbf{+500:} Granted for collecting a gift.
    \item \textbf{-50:} Penalizes the agent for missing a gift.
    \item \textbf{-1000:} Imposed when the bird collides with a pipe or the ground, leading to the termination of the episode.
\end{itemize}
This reward structure balances positive reinforcement for desirable actions with penalties for mistakes, thereby shaping the agent’s behavior effectively.

\section{Methodology}
\subsection{Environment and Agent Architecture}
The environment is simulated using \texttt{pygame}, which manages state updates, applies action effects, and assigns rewards based on the bird’s interactions. The agent employs a Q-table, stored as a dictionary where each state tuple is associated with an array of Q-values for the available actions. States are added dynamically as they are encountered, and actions are chosen based on either a greedy or \(\epsilon\)-greedy strategy, with the exploration rate decaying over time to shift the focus from exploration to exploitation.

\subsection{Learning Algorithms}
Both Q-learning and SARSA are implemented within the same framework for state discretization and Q-table management. Their primary difference lies in the update mechanism:
\begin{itemize}
    \item \textbf{Q-learning:} An off-policy method that updates Q-values using the maximum expected future reward from the next state. This approach typically converges faster, though it may promote riskier strategies.
    \item \textbf{SARSA:} An on-policy method that updates Q-values based on the actual action taken in the next state, leading to a more conservative and stable learning process.
\end{itemize}
Key parameters such as the learning rate (\(\alpha\)), discount factor (\(\gamma\)), and exploration rate (\(\epsilon\)) play critical roles in both algorithms.

\section{Experimental Evaluation}
\subsection{Setup}
The training process is configured with the following parameters:
\begin{itemize}
    \item Learning rate (\(\alpha\)): 0.1
    \item Discount factor (\(\gamma\)): 0.9
    \item Exploration rate (\(\epsilon\)): Initially high and gradually decaying
    \item Number of training episodes: 1000
\end{itemize}

\subsection{Performance Metrics}
The agent’s performance is evaluated based on:
\begin{itemize}
    \item \textbf{Average Score per Episode:} Indicative of the overall performance.
    \item \textbf{Convergence Speed:} Reflected by the stabilization of Q-values across episodes.
    \item \textbf{Policy Stability:} Measured by the consistency of the agent's decisions during testing.
    \item \textbf{Bonus Collection Frequency:} The frequency of collecting gifts, which reflects the effectiveness of the reward function.
\end{itemize}

\subsection{Results and Discussion}
Preliminary observations indicate that:
\begin{itemize}
    \item Q-learning converges faster but may adopt riskier strategies.
    \item SARSA provides a more stable learning process with fewer collisions due to its on-policy update mechanism.
    \item The enhanced reward function is critical in guiding the agent toward effective behavior.
\end{itemize}

\section{Discussion and Conclusion}
\subsection{Discussion}
A detailed comparison between Q-learning and SARSA reveals:
\begin{itemize}
    \item \textbf{Q-learning:} Its off-policy nature and optimistic estimation of future rewards lead to rapid convergence but may encourage riskier actions.
    \item \textbf{SARSA:} By basing updates on the actual actions taken, SARSA tends to foster a more conservative and stable policy.
    \item The multidimensional feedback provided by the enhanced reward function is essential for effective learning.
\end{itemize}

\subsection{Limitations and Future Work}
While the results are promising, several limitations remain:
\begin{itemize}
    \item The discretization of continuous states might omit some dynamic details of the game.
    \item Further parameter tuning may improve performance.
    \item Future work could explore more complex reward structures and advanced state representation techniques, possibly incorporating deep learning methods.
\end{itemize}

\subsection{Conclusion}
This report demonstrates that both Q-learning and SARSA can effectively train an agent in the Flappy Bird game using an enhanced reward scheme. Q-learning offers faster convergence, while SARSA provides a safer and more stable policy. The analysis provides a strong foundation for further improvements and deeper research into reinforcement learning strategies.

%\begin{thebibliography}{9}
%    \bibitem{sutton-barto}
%      Sutton, R. S. and Barto, A. G.,
%      \textit{Reinforcement Learning: An Introduction}.
%      MIT Press, 2018.
%    
%    \bibitem{mnih2015human}
%      Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., et al.,
%      ``Human-level control through deep reinforcement learning,''
%      \textit{Nature}, 518(7540), 529--533, 2015.
%\end{thebibliography}

\end{document}
